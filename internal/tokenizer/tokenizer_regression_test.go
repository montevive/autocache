package tokenizer

import (
	"encoding/json"
	"fmt"
	"os"
	"sync"
	"testing"

	"autocache/internal/types"

	"github.com/sirupsen/logrus"
)

// TestUnicodeOrdinalIndicators tests Spanish ordinal indicators that caused the original panic
func TestUnicodeOrdinalIndicators(t *testing.T) {
	testCases := []struct {
		name string
		text string
	}{
		{"Masculine ordinal", "1Âº paso"},
		{"Feminine ordinal", "1Âª vez"},
		{"Multiple ordinals", "1Âº paso, 2Âº intento, 3Âº lugar"},
		{"Production text", "Necesito que generes un informe diario de ayer. Ayer fue 3 octubre 2025. Quiero: 1Âº AnÃ¡lisis de las visitas de la web ayer. 2Âº Â¿QuÃ© eventos tenemos hoy, maÃ±ana y pasado maÃ±ana?, 3Âº Â¿QuÃ© tareas tenemos pendientes?"},
	}

	logger := logrus.New()
	logger.SetLevel(logrus.FatalLevel)

	// Test with all tokenizers
	tokenizers := map[string]Tokenizer{}

	anthropic, err := NewAnthropicRealTokenizerWithLogger(logger)
	if err != nil {
		t.Logf("Skipping anthropic tokenizer: %v", err)
	} else {
		tokenizers["anthropic"] = anthropic
	}

	offline, err := NewOfflineTokenizerWithLogger(logger)
	if err != nil {
		t.Logf("Skipping offline tokenizer: %v", err)
	} else {
		tokenizers["offline"] = offline
	}

	tokenizers["heuristic"] = NewAnthropicTokenizer()

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			for name, tokenizer := range tokenizers {
				// Should not panic
				count := tokenizer.CountTokens(tc.text)

				if count <= 0 {
					t.Errorf("%s: got non-positive count %d for %q", name, count, tc.text)
				}

				t.Logf("%s: %q = %d tokens", name, truncateForLog(tc.text, 50), count)
			}

			// Check offline tokenizer panic stats
			if offlineTokenizer, ok := tokenizers["offline"].(*OfflineTokenizer); ok {
				stats := offlineTokenizer.GetPanicStats()
				if stats["panic_count"] > 0 {
					t.Logf("Note: Offline tokenizer had %d panics (recovered with fallback)", stats["panic_count"])
				}
			}
		})
	}
}

// TestUnicodeAccentedCharacters tests Spanish accented characters
func TestUnicodeAccentedCharacters(t *testing.T) {
	testTexts := []string{
		"Ã¡ Ã© Ã­ Ã³ Ãº",
		"Ã Ã‰ Ã Ã“ Ãš",
		"Ã± Ã‘",
		"Â¿CÃ³mo estÃ¡s?",
		"Â¡Muy bien!",
		"El niÃ±o comiÃ³ maÃ±ana en el jardÃ­n.",
		"JosÃ© MarÃ­a GonzÃ¡lez PÃ©rez",
		"InformaciÃ³n tÃ©cnica y anÃ¡lisis crÃ­tico",
	}

	logger := logrus.New()
	logger.SetLevel(logrus.FatalLevel)

	// Test anthropic tokenizer (should never panic)
	anthropic, err := NewAnthropicRealTokenizerWithLogger(logger)
	if err != nil {
		t.Skipf("Skipping test: failed to create tokenizer: %v", err)
	}

	for _, text := range testTexts {
		t.Run(text, func(t *testing.T) {
			// Should not panic
			count := anthropic.CountTokens(text)

			if count <= 0 {
				t.Errorf("Got non-positive count %d for %q", count, text)
			}

			t.Logf("%q = %d tokens", text, count)
		})
	}
}

// TestUnicodeStressTest tests a wide variety of Unicode characters
func TestUnicodeStressTest(t *testing.T) {
	testCases := []struct {
		name string
		text string
	}{
		{"Superscripts", "xÂ¹ xÂ² xÂ³ xâ¿"},
		{"Subscripts", "Hâ‚‚O COâ‚‚ CHâ‚„"},
		{"Math symbols", "âˆ‘ âˆ« âˆ‚ âˆ‡ âˆ â‰ˆ â‰  â‰¤ â‰¥"},
		{"Greek letters", "Î± Î² Î³ Î´ Îµ Î¶ Î· Î¸"},
		{"Currency", "$ â‚¬ Â£ Â¥ â‚¹ â‚½ Â¢"},
		{"Arrows", "â† â†’ â†‘ â†“ â†” â‡’ â‡"},
		{"Emoji simple", "ğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ˜ ğŸ˜†"},
		{"Emoji complex", "ğŸ‘¨â€ğŸ’» ğŸ‘©â€ğŸ”¬ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦"},
		{"Chinese", "ä½ å¥½ä¸–ç•Œ"},
		{"Japanese", "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ"},
		{"Arabic", "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"},
		{"Russian", "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€"},
		{"Mixed", "Hello ä¸–ç•Œ ğŸŒ Ù…Ø±Ø­Ø¨Ø§ ĞŸÑ€Ğ¸Ğ²ĞµÑ‚"},
	}

	logger := logrus.New()
	logger.SetLevel(logrus.FatalLevel)

	anthropic, err := NewAnthropicRealTokenizerWithLogger(logger)
	if err != nil {
		t.Skipf("Skipping test: failed to create tokenizer: %v", err)
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			// Should not panic
			count := anthropic.CountTokens(tc.text)

			if count < 0 {
				t.Errorf("Got negative count %d for %q", count, tc.text)
			}

			t.Logf("%s: %q = %d tokens", tc.name, tc.text, count)
		})
	}
}

// TestProductionPanicScenarios tests actual scenarios that caused panics in production
func TestProductionPanicScenarios(t *testing.T) {
	// Real production texts from logs
	productionTexts := []string{
		"Necesito que generes un informe diario de ayer. Ayer fue 3 octubre 2025. Quiero: 1Âº AnÃ¡lisis de las visitas de la web ayer. 2Âº Â¿QuÃ© eventos tenemos hoy, maÃ±ana y pasado maÃ±ana?, 3Âº Â¿QuÃ© tareas tenemos pendientes?",
		"Por favor, realiza: 1Âº RevisiÃ³n del cÃ³digo, 2Âº Pruebas unitarias, 3Âº DocumentaciÃ³n",
		"Objetivos: 1Âº Aumentar ventas, 2Âº Mejorar satisfacciÃ³n, 3Âº Reducir costos",
	}

	logger := logrus.New()
	logger.SetLevel(logrus.ErrorLevel)

	// Test all tokenizers
	tokenizers := map[string]Tokenizer{}

	anthropic, err := NewAnthropicRealTokenizerWithLogger(logger)
	if err != nil {
		t.Logf("Skipping anthropic tokenizer: %v", err)
	} else {
		tokenizers["anthropic"] = anthropic
	}

	offline, err := NewOfflineTokenizerWithLogger(logger)
	if err != nil {
		t.Logf("Skipping offline tokenizer: %v", err)
	} else {
		tokenizers["offline"] = offline
	}

	tokenizers["heuristic"] = NewAnthropicTokenizer()

	for i, text := range productionTexts {
		t.Run(fmt.Sprintf("Production scenario %d", i+1), func(t *testing.T) {
			for name, tokenizer := range tokenizers {
				// Process text (should not panic)
				count := tokenizer.CountTokens(text)

				if count <= 0 {
					t.Errorf("%s: got non-positive count %d", name, count)
				}

				t.Logf("%s: %d tokens", name, count)
			}

			// Verify anthropic tokenizer had 0 panics
			if anthropicTokenizer, ok := tokenizers["anthropic"].(*AnthropicRealTokenizer); ok {
				// Anthropic tokenizer doesn't have panic recovery, so getting here means no panic
				_ = anthropicTokenizer
				t.Logf("Anthropic tokenizer: âœ“ No panics")
			}

			// Check offline tokenizer stats
			if offlineTokenizer, ok := tokenizers["offline"].(*OfflineTokenizer); ok {
				stats := offlineTokenizer.GetPanicStats()
				if stats["panic_count"] > 0 {
					t.Logf("Offline tokenizer: %d panics (recovered successfully)", stats["panic_count"])
				} else {
					t.Logf("Offline tokenizer: âœ“ No panics")
				}
			}
		})
	}
}

// TestConcurrentUnicodeStress tests concurrent processing of Unicode text
func TestConcurrentUnicodeStress(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping stress test in short mode")
	}

	problematicTexts := []string{
		"1Âº 2Âº 3Âº 4Âº 5Âº",
		"Ã¡ Ã© Ã­ Ã³ Ãº Ã±",
		"Â¿CÃ³mo estÃ¡s? Â¡Muy bien!",
		"ä½ å¥½ä¸–ç•Œ ã“ã‚“ã«ã¡ã¯",
		"ğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ‘¨â€ğŸ’» ğŸ‘©â€ğŸ”¬",
	}

	logger := logrus.New()
	logger.SetLevel(logrus.FatalLevel)

	anthropic, err := NewAnthropicRealTokenizerWithLogger(logger)
	if err != nil {
		t.Skipf("Skipping test: failed to create tokenizer: %v", err)
	}

	const numGoroutines = 100
	const iterationsPerGoroutine = 10

	var wg sync.WaitGroup
	wg.Add(numGoroutines)

	errors := make(chan error, numGoroutines*iterationsPerGoroutine)

	for i := 0; i < numGoroutines; i++ {
		go func(id int) {
			defer wg.Done()

			for j := 0; j < iterationsPerGoroutine; j++ {
				text := problematicTexts[j%len(problematicTexts)]

				// This should not panic
				count := anthropic.CountTokens(text)

				if count < 0 {
					errors <- fmt.Errorf("goroutine %d iteration %d: got negative count %d for %q",
						id, j, count, text)
				}
			}
		}(i)
	}

	wg.Wait()
	close(errors)

	// Check for errors
	errorCount := 0
	for err := range errors {
		t.Error(err)
		errorCount++
	}

	if errorCount == 0 {
		t.Logf("âœ“ Successfully processed %d concurrent Unicode operations with 0 panics",
			numGoroutines*iterationsPerGoroutine)
	}
}

// truncateForLog truncates text for logging
func truncateForLog(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}

// TestRegressionN8NWorkflow verifies the n8n workflow doesn't cause panics with new tokenizer
func TestRegressionN8NWorkflow(t *testing.T) {
	// Load n8n test data
	inputData, err := os.ReadFile("test_data/scenarios/n8n_agent_workflow/input.json")
	if err != nil {
		t.Skipf("Skipping test: n8n test data not found: %v", err)
	}

	var req types.AnthropicRequest
	if err := json.Unmarshal(inputData, &req); err != nil {
		t.Fatalf("Failed to parse n8n input: %v", err)
	}

	logger := logrus.New()
	logger.SetLevel(logrus.ErrorLevel)

	// Test with anthropic tokenizer (should never panic)
	anthropic, err := NewAnthropicRealTokenizerWithLogger(logger)
	if err != nil {
		t.Skipf("Skipping test: failed to create tokenizer: %v", err)
	}

	// Process the request (should not panic)
	totalTokens := anthropic.EstimateRequestTokens(&req)

	if totalTokens <= 0 {
		t.Errorf("Got non-positive token count: %d", totalTokens)
	}

	t.Logf("âœ“ N8N workflow processed successfully: %d tokens, 0 panics", totalTokens)

	// Test each tool individually
	t.Logf("Processing %d tools...", len(req.Tools))
	for i, tool := range req.Tools {
		tokens := anthropic.CountToolTokens(tool)
		if tokens <= 0 {
			t.Errorf("Tool %d (%s) has non-positive token count: %d", i, tool.Name, tokens)
		}
	}

	t.Logf("âœ“ All %d tools processed without panics", len(req.Tools))
}
