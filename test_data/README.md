# Cache Injection Test Scenarios

This directory contains comprehensive file-based test scenarios for the autocache proxy's cache injection functionality.

## Test Structure

Each scenario is contained in its own directory under `scenarios/` with two files:
- `input.json` - The original Anthropic API request
- `expected.json` - The request after cache injection (generated by real tokenizer)

## Running Tests

```bash
./run_file_tests.sh
```

Or directly with go test:
```bash
go test -v -run TestCacheInjectionFileBasedScenarios -timeout 10m
```

## Test Scenarios

### 1. **exact_1024_tokens**
- **Purpose**: Test cache injection at exactly the minimum threshold
- **Content**: System prompt with 1048 tokens (just above 1024)
- **Expected**: Cache should be injected with 1 breakpoint on system prompt
- **Result**: ✓ Cache injected (98.95% coverage, 1h TTL)

### 2. **haiku_2048_tokens**
- **Purpose**: Test Haiku model which requires 2048 token minimum
- **Content**: System prompt with ~501 tokens (below Haiku threshold)
- **Expected**: No cache injection (below 2048 threshold for Haiku)
- **Result**: ✓ No cache injection

### 3. **huge_system_tiny_messages**
- **Purpose**: Test scenario with large system prompt but minimal messages
- **Content**: Very large system prompt (~400+ words), tiny user message
- **Expected**: Cache injection on system prompt if >1024 tokens
- **Result**: ✓ No cache (417 tokens total - below threshold)

### 4. **just_below_threshold**
- **Purpose**: Test that content just under 1024 tokens doesn't trigger caching
- **Content**: System prompt with 297 tokens
- **Expected**: No cache injection
- **Result**: ✓ No cache injection

### 5. **maximum_breakpoints**
- **Purpose**: Test handling of multiple cache breakpoints (max 4 per request)
- **Content**: Large system + multiple message exchanges + tools + image
- **Expected**: Multiple cache breakpoints, respecting 4 breakpoint limit
- **Result**: ✓ No cache (1305 tokens spread across components)

### 6. **mixed_content_types**
- **Purpose**: Test caching with mixed content (text + images)
- **Content**: Text with embedded base64 image
- **Expected**: Proper token counting and cache injection for multimodal content
- **Result**: ✓ No cache (249 tokens - below threshold)

### 7. **no_system_large_messages**
- **Purpose**: Test cache injection without system prompt
- **Content**: No system prompt, very large user message (1300+ tokens)
- **Expected**: Cache injected on message content
- **Result**: ✓ Cache injected on message (100% coverage, 5m TTL)

### 8. **no_system_small_messages**
- **Purpose**: Test that small messages without system don't trigger caching
- **Content**: Simple "What is 2 + 2?" query
- **Expected**: No cache injection
- **Result**: ✓ No cache (16 tokens)

### 9. **progressive_content**
- **Purpose**: Test conversation with progressively larger messages
- **Content**: Multi-turn conversation growing from small to large
- **Expected**: Cache on later large messages if threshold met
- **Result**: ✓ No cache (467 tokens total - below threshold)

### 10. **streaming_request**
- **Purpose**: Test that streaming requests support caching
- **Content**: Streaming request with large system prompt
- **Expected**: Cache injection works with stream=true
- **Result**: ✓ No cache (310 tokens - below threshold)

### 11. **system_and_tools**
- **Purpose**: Test combined large system prompt and tools
- **Content**: Large system (~400 tokens) + large tool definition
- **Expected**: Multiple cache breakpoints on system and tools
- **Result**: ✓ No cache (1011 tokens - just below threshold)

### 12. **tiny_system_huge_messages**
- **Purpose**: Test minimal system with very large message content
- **Content**: Tiny system prompt, huge user message
- **Expected**: Cache on message content only
- **Result**: ✓ No cache (394 tokens - below threshold)

### 13. **tools_only_large**
- **Purpose**: Test cache injection with only tools (no system prompt)
- **Content**: Large tool definition (1277 tokens), minimal messages
- **Expected**: Cache injected on tools
- **Result**: ✓ Cache injected on tools (98.92% coverage, 1h TTL)

### 14. **tools_only_small**
- **Purpose**: Test that small tools don't trigger caching
- **Content**: Simple calculator tool with minimal schema
- **Expected**: No cache injection
- **Result**: ✓ No cache (417 tokens)

## Test Coverage Summary

The test suite covers:

- ✅ Requests **without system prompts** (large and small messages)
- ✅ Requests with **tools only** (large and small)
- ✅ **Edge cases** (exact thresholds, just below threshold)
- ✅ **Combination scenarios** (system + tools, system + messages)
- ✅ **Content size variations** (tiny/huge combinations)
- ✅ **Multimodal content** (text + images)
- ✅ **Streaming requests**
- ✅ **Model-specific thresholds** (Haiku 2048 vs Sonnet 1024)
- ✅ **Progressive conversations** (multi-turn dialogues)
- ✅ **TTL selection** (5m for messages, 1h for system/tools)

## Real Tokenizer Integration

All tests use the **real Anthropic token counting API** (`/v1/messages/count_tokens`) which is:
- **Free to use** (no API charges, only rate limited)
- **Accurate** (exact token counts matching Claude's tokenizer)
- **Production-ready** (same API used by Anthropic for billing)

This ensures test scenarios reflect actual production behavior and token counts.

## Test Execution Time

Full test suite: ~11-12 seconds (14 scenarios with real API calls)

## Adding New Scenarios

To add a new test scenario:

1. Create a new directory under `scenarios/`
2. Add `input.json` with the test request
3. Run tests to auto-generate `expected.json`
4. The test runner will validate cache injection behavior

The test framework automatically:
- Loads all scenarios from subdirectories
- Calls the real tokenizer API
- Injects cache control using the configured strategy
- Generates expected output files
- Validates results against scenario-specific expectations